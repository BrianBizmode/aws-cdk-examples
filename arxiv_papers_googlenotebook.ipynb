{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianBizmode/aws-cdk-examples/blob/master/arxiv_papers_googlenotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arxivから論文のアブストラクトをダウンロードします。"
      ],
      "metadata": {
        "id": "uhXa5hfmbrtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備(pip)"
      ],
      "metadata": {
        "id": "1qdogGaGMwmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install arxiv datasets\n",
        "!pip -q install markdown"
      ],
      "metadata": {
        "id": "e7kG8SgvZE-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "7xA9eZJOl8hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 入力項目\n",
        "# @markdown 入手したいArXiv論文概要.\n",
        "\n",
        "category = 'cat:cs' # @param {type: \"string\"}\n",
        "input_search_query = 'Retrieval Augmented Generation' # @param {type: \"string\"}\n",
        "memo_tilte = 'Recent_LLM_paper'  # @param {type: \"string\"}\n",
        "num_results = 100  # @param {type: \"number\"}\n",
        "# date = '2010-11-05'  # @param {type: \"date\"}\n",
        "sort_by = \"SubmittedDate\"  # @param ['SubmittedDate', 'Relevance']\n",
        "save_path = 'notebooklm' # @param {type: \"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GM3NZxEnI768"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bjeuDmyhu5R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VBbw74GCeZ-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_filename = \"Papers_about_\" + input_search_query.replace(\" \", \"_\")\n",
        "search_query = category + \" \" + input_search_query\n",
        "sort_by = arxiv.SortCriterion.SubmittedDate if sort_by == \"SubmittedDate\" else arxiv.SortCriterion.Relevance"
      ],
      "metadata": {
        "id": "tdkwyi9jNtAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_save_path(gdrive_path, save_path):\n",
        "    if os.path.exists('/content/drive'):  # Google Driveがマウントされているか確認\n",
        "        return os.path.join(gdrive_path, save_path) + \"/\"  # Google Driveのパスを返す\n",
        "    else:\n",
        "        return save_path + \"/\"  # ローカルのパスを返す\n",
        "\n",
        "gdrive_path = '/content/drive/My Drive/'  # Google Driveのパス\n",
        "save_path = get_save_path(gdrive_path, save_path)  # 保存パスを取得\n",
        "print(save_path)  # 保存パスを表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3tyS1CZtpi",
        "outputId": "ef1430a4-cb52-4f4d-dbce-76ab1f92f069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/notebooklm/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ArXivからダウンロード"
      ],
      "metadata": {
        "id": "e-Yo__kOM_Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from arxiv import UnexpectedEmptyPageError\n",
        "\n",
        "# Arxiv APIを使用して論文を検索します。\n",
        "query = search_query\n",
        "\n",
        "# arxiv.Clientをインスタンス化します。\n",
        "client = arxiv.Client()\n",
        "\n",
        "# 検索条件を設定します。\n",
        "search = arxiv.Search(\n",
        "    query=query,\n",
        "    max_results=100,  # 取得する論文の最大件数を指定します。\n",
        "    sort_by=sort_by\n",
        ")\n",
        "\n",
        "# 検索を実行し、結果を取得します。\n",
        "results = []\n",
        "try:\n",
        "    for result in client.results(search): # client.results()を使用\n",
        "        results.append({\n",
        "            \"title\": result.title,\n",
        "            \"abstract\": result.summary,\n",
        "            #Bibtexを生成するために必要な情報を追加\n",
        "            'authors': result.authors,\n",
        "            'published': result.published,\n",
        "            'journal_ref': result.journal_ref,\n",
        "            'entry_id': result.entry_id\n",
        "        })\n",
        "except UnexpectedEmptyPageError as e:\n",
        "    # エラーが発生した場合、エラーメッセージと発生したURLを表示します\n",
        "    print(f\"Error: {e}\")\n",
        "    print(f\"URL: {e.url}\")\n",
        "    # 処理を継続するか、終了するかを決定します\n",
        "    # ここでは、エラーが発生したページをスキップして処理を継続します\n",
        "\n",
        "\n",
        "# BibTeX形式の引用を生成する関数\n",
        "def generate_bibtex(result):\n",
        "    authors = \" and \".join([author.name for author in result['authors']])\n",
        "    title = result['title']\n",
        "    year = result['published'].year\n",
        "    if result['journal_ref'] is not None:\n",
        "        journal = result['journal_ref']\n",
        "    else:\n",
        "        journal = result['entry_id']\n",
        "    bibtex = (\n",
        "          f\"@article{{{result['entry_id']}}},\\n\"\n",
        "          f\"author = {{{authors}}},\\n\"\n",
        "          f\"title = {{{title}}},\\n\"\n",
        "          f\"journal = {{{journal}}},\\n\"\n",
        "          f\"year = {{{year}}}\\n\"\n",
        "          \"}\"\n",
        "      )\n",
        "    return bibtex\n",
        "\n",
        "\n",
        "# BibTeX形式の引用を結果に追加します。\n",
        "for result in results:\n",
        "    result[\"bibtex\"] = generate_bibtex(result)\n",
        "\n",
        "# データフレームに変換します。\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# title, abstract, bibtexだけのDataFrameを作成します。\n",
        "df = df[[\"title\", \"abstract\", \"bibtex\"]]\n",
        "\n",
        "# CSVファイルとして保存します。\n",
        "df.to_csv(\"arxiv_papers.csv\", index=False)\n",
        "\n",
        "print(\"データセットが 'arxiv_papers.csv' として一時保存されました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-qrzq0Epmvz",
        "outputId": "e12cc9ae-d8bd-4c24-c5ad-df8603dd9878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データセットが 'arxiv_papers.csv' として一時保存されました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"arxiv_papers.csv\")"
      ],
      "metadata": {
        "id": "4jM-481Hq6Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import markdown\n",
        "\n",
        "# 初期\n",
        "max_chars = 50000 # NotebookLMの1ファイルあたりの許容文字数\n",
        "current_chars = 0\n",
        "file_number = 1\n",
        "html_content = \"\"\n",
        "to_markdown = True\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "for index in range(len(df)):\n",
        "    content = f\"\"\"# Title: {df[\"title\"][index]}\n",
        "## abstract\n",
        "{df[\"abstract\"][index]}\n",
        "## bibtex\n",
        "{df['bibtex'][index]}\n",
        "\"\"\"\n",
        "    if to_markdown == True:\n",
        "        html_entry = content\n",
        "        ext = \".md\"\n",
        "    else:\n",
        "        html_entry = markdown.markdown(content)\n",
        "        ext = \".html\"\n",
        "\n",
        "    entry_length = len(html_entry)\n",
        "\n",
        "    if current_chars + entry_length > max_chars:\n",
        "        # If character limit is exceeded, save current content and start a new file\n",
        "        with open(f\"{save_path}{base_filename}_part{file_number}{ext}\", \"w\") as f:\n",
        "            f.write(html_content)\n",
        "        print(f\"{base_filename}_part{file_number}{ext} saved.\")\n",
        "\n",
        "        file_number += 1\n",
        "        html_content = \"\"  # Reset for the new file\n",
        "        current_chars = 0\n",
        "\n",
        "    html_content += html_entry\n",
        "    current_chars += entry_length\n",
        "\n",
        "# Save the remaining content to the last file\n",
        "if html_content:\n",
        "    with open(f\"{save_path}{base_filename}_part{file_number}{ext}\", \"w\") as f:\n",
        "        f.write(html_content)\n",
        "    print(f\"{save_path}{file_number}{ext} saved.\")\n",
        "\n",
        "print(\"HTML/md content split and saved into multiple files.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c0njk0cIbSg",
        "outputId": "82349786-9295-41e8-a1ab-0ee64b4c1c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Papers_about_Retrieval_Augmented_Generation_part1.md saved.\n",
            "Papers_about_Retrieval_Augmented_Generation_part2.md saved.\n",
            "Papers_about_Retrieval_Augmented_Generation_part3.md saved.\n",
            "arxiv_papers_part4.md saved.\n",
            "HTML/md content split and saved into multiple files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_path):\n",
        "    \"\"\"Zips a folder and its contents.\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to the folder to zip.\n",
        "        output_path: Path to the output zip file.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "# Replace 'save_path' with the actual path to your folder\n",
        "zip_folder(save_path, '/content/archive.zip')\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download(\"/content/archive.zip\")\n",
        "\n",
        "print(f\"{save_path} zipped and downloaded as archive.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "oujP10hrdD_w",
        "outputId": "eb24bd95-ef5a-4d5b-ebab-cc096bb6d0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9131a94e-78a3-42fc-b3da-df87d50037a2\", \"archive.zip\", 60521)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/notebooklm/ zipped and downloaded as archive.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Flashでまとめてみる"
      ],
      "metadata": {
        "id": "T4mYm1-YcOFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU google-generativeai"
      ],
      "metadata": {
        "id": "wYzBVG68lHHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faile_path = f\"{save_path}{base_filename}_part{1}{ext}\"\n",
        "with open(faile_path, 'r') as file:\n",
        "  text = file.read()"
      ],
      "metadata": {
        "id": "JHVyJm-XualB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "AMlyFN9XoIFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "model_name = 'gemini-1.5-flash-latest'\n",
        "genai.configure(api_key=API_KEY)\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "\n",
        "\n",
        "def summarize_text(text):\n",
        "\n",
        "  my_prompt = f\"\"\"<Inputs>\n",
        "<search_query>{input_search_query}</search_query>\n",
        "</Inputs>\n",
        "<Instructions Structure>\n",
        "1. まず、{input_search_query}に関する論文のリストを受け取ります。\n",
        "2. 次に、そのリストをもとに最近の技術傾向を要約します。\n",
        "3. 最後に、要約を大学生でも分かりやすいように日本語で提供します。\n",
        "</Instructions Structure>\n",
        "<Instructions>\n",
        "以下の内容に従って、{input_search_query}に関する最近の技術傾向をまとめてください。\n",
        "提供された論文のリストを読み、各論文の主要な技術的進展やトレンドを特定します。\n",
        "特定したトレンドを基に、最近の技術傾向について簡潔に要約します。\n",
        "最後に、その要約を日本語で記述してください。\n",
        "論文のリストは以下の通りです：\n",
        "<論文リスト>\n",
        "{text}\n",
        "</論文リスト>\n",
        "</Instructions>\n",
        "\"\"\"\n",
        "\n",
        "  response = model.generate_content(my_prompt)\n",
        "  return response\n",
        "\n",
        "# 実行\n",
        "summary = summarize_text(text)\n",
        "\n",
        "# 要約の出力\n",
        "print(summary.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "ie5bC6a3k8Cg",
        "outputId": "63ed07ce-7ec1-4212-a228-897fc8c80b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Retrieval Augmented Generation の最近の技術トレンド\n",
            "\n",
            "提供された論文リストから、Retrieval Augmented Generation（RAG）に関する最近の技術トレンドを以下のように要約することができます。\n",
            "\n",
            "**1. 汎用性と効率性の向上:**\n",
            "\n",
            "* **URAvatar** は、携帯電話スキャンから現実的なアバターを作成する手法を提案し、照明条件に依存しない汎用的なアバター生成を実現しました。リアルタイムレンダリングとグローバルイルミネーションの効率的な統合によって、より自然で没入感のあるアバター体験を可能にします。\n",
            "* **EgoMimic** は、人間のエゴセントリックビデオデータを活用することで、ロボットの操作スキルを向上させるイミテーションラーニングフレームワークです。人間の動きをより直接的にロボットに適用することで、複雑なタスクの学習をより効率的に行うことができます。\n",
            "* **TabM** は、パラメータ効率的なアンサンブル手法により、表形式データに対する深層学習モデルの精度と効率性を向上させます。従来の深層学習モデルに比べて、少ないパラメータで高い性能を実現することが可能です。\n",
            "\n",
            "**2. より自然で複雑な生成:**\n",
            "\n",
            "* **DEMO** は、テキストからビデオを生成する際に、テキストエンコーディングと条件付けをそれぞれコンテンツとモーションに分解することで、より自然な動きを持つビデオを生成します。テキストの解釈と生成を強化し、静的な出力の問題を解消するアプローチです。\n",
            "* **DiffPano** は、大規模なパノラマビデオテキストデータセットと球面エピポーラ対応の拡散モデルを活用することで、テキストから一貫性のある多様なパノラマ画像を生成します。360度の画像生成を可能にし、より没入感のあるVR/AR体験を提供します。\n",
            "* **LingGen** は、パワーズ法マスクを使用した新しい制御可能なテキスト生成手法です。複数の言語属性を同時に制御することができ、より多様で精度の高いテキストを生成することができます。\n",
            "\n",
            "**3. より正確なモデリングと理解:**\n",
            "\n",
            "* **Geometric Diffusion Bridge (GDB)** は、幾何学的状態の進化を正確に予測するための新しい生成モデルフレームワークです。確率的なアプローチにより、初期状態と目標状態を橋渡しし、複雑なシステムにおける幾何学的状態の変化を正確にモデル化します。\n",
            "* **GeoSplatting** は、3Dガウシアン・スプラッティングに明示的な幾何学的ガイダンスと微分可能なPBR方程式を追加することで、物理的に正確な逆レンダリングを実現する手法です。従来の3DGS手法よりも精度の高い表面ノーマルのモデリングが可能になります。\n",
            "* **SelfCodeAlign** は、コード生成のための新しい自己整合学習パイプラインです。人間の注釈なしで、大規模言語モデルがコード生成の指示をより良く理解できるようにします。\n",
            "\n",
            "**4. 効率性と一般化能力の向上:**\n",
            "\n",
            "* **Learning Video Representations without Natural Videos** は、自然動画を使わずに合成動画と自然画像から動画表現を学習する手法です。自然動画データの利用に制限されることなく、より効率的かつ柔軟な動画表現学習を可能にします。\n",
            "* **Understanding Optimization in Deep Learning with Central Flows** は、深層学習における最適化をより深く理解するための新しいツールであるセントラルフローを導入します。最適化の挙動をモデル化し、学習過程の理解を深めることができます。\n",
            "* **Thought Space Explorer (TSE)** は、大規模言語モデルの推論能力を向上させるための新しいフレームワークです。思考空間を拡張し、より複雑な推論タスクを解決することを可能にします。\n",
            "\n",
            "これらのトレンドは、Retrieval Augmented Generation の分野が、より汎用性、効率性、自然さ、正確さを追求し、さまざまな分野での応用範囲を拡大していくことを示唆しています。\n",
            "\n",
            "**大学生にもわかりやすい説明:**\n",
            "\n",
            "Retrieval Augmented Generation（RAG）は、外部知識ベースから関連情報を検索して、より質の高い文章やコンテンツを生成する技術です。近年、この分野では、以下のような技術的な進歩が見られます。\n",
            "\n",
            "* リアルタイムレンダリングや人間の動きを直接活用するなど、効率的で汎用性の高いモデルが開発されています。\n",
            "* 複雑な言語属性を制御できるようになったり、テキストからより自然な動きを持つ動画を生成できるようになったりと、生成の質が向上しています。\n",
            "* 物理的な法則を考慮したモデリングや、大規模言語モデルの思考能力を向上させるための研究が進められています。\n",
            "\n",
            "これにより、RAGは、より自然で洗練された文章の生成、よりインタラクティブなコンテンツの創作、複雑なデータ分析など、さまざまな分野で活用される可能性を秘めています。\n"
          ]
        }
      ]
    }
  ]
}